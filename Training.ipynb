{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "snowball = SnowballStemmer(\"german\")\n",
    "\n",
    "with open(\"qa.json\", encoding=\"utf8\") as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "labels = []\n",
    "questions = []\n",
    "answers_y = []\n",
    "\n",
    "#Tokenize and read Words, Labels, Docs\n",
    "for intent in data[\"qa\"]:\n",
    "    for pattern in intent[\"questions\"]:\n",
    "        tokenized_words = nltk.word_tokenize(pattern, language='german')\n",
    "        words.extend(tokenized_words)\n",
    "        \n",
    "        questions.append(pattern)\n",
    "        answers_y.append(intent[\"tag\"])\n",
    "        \n",
    "        if intent[\"tag\"] not in labels:\n",
    "            labels.append(intent[\"tag\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "training_x = []\n",
    "training_y = []\n",
    "\n",
    "label_bag = [0 for i in range(len(labels))]\n",
    "\n",
    "for i, question in enumerate(questions):\n",
    "    #tokenize\n",
    "    tokens = nltk.word_tokenize(question, language='german')\n",
    "    #lowercase\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    #remove punctution\n",
    "    table = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    #remove non-alphabetic/non-numeric\n",
    "    real_tokens = [token for token in stripped if token.isalpha() or token.isnumeric()]\n",
    "    #stemming\n",
    "    sequence = [snowball.stem(token) for token in real_tokens]\n",
    "    \n",
    "    training_x.append(sequence)\n",
    "    \n",
    "    #labels to one-hot-encoded labels\n",
    "    output = label_bag[:]\n",
    "    output[labels.index(answers_y[i])] = 1\n",
    "    training_y.append(output)\n",
    "\n",
    "training_y = np.array(training_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "[[  0   0   0 ...   0   0  37]\n",
      " [  0   0   0 ...   9  38  39]\n",
      " [  0   0   0 ...   0   0  40]\n",
      " ...\n",
      " [  0   0   0 ...   0  16   5]\n",
      " [  0   0   0 ...   0   0  99]\n",
      " [  0   0   0 ...   0  36 100]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(training_x)\n",
    "sequences = tokenizer.texts_to_sequences(training_x)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(len(word_index))\n",
    "\n",
    "padded_sequences = pad_sequences(sequences, maxlen=300)\n",
    "print(padded_sequences)\n",
    "\n",
    "num_words = len(word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\dima-PC\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 300, 300)          30300     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 14)                910       \n",
      "=================================================================\n",
      "Total params: 124,650\n",
      "Trainable params: 124,650\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Flatten, LSTM, Input, GlobalMaxPooling1D\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.initializers import Constant\n",
    "from keras.models import Model\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, 300, input_length=300))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(len(labels), activation='softmax'))\n",
    "print(model.summary())\n",
    "\n",
    "# compile network\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\dima-PC\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "54/54 [==============================] - 38s 708ms/step - loss: 2.5979 - acc: 0.1296\n",
      "Epoch 2/100\n",
      "54/54 [==============================] - 24s 438ms/step - loss: 2.3543 - acc: 0.2037\n",
      "Epoch 3/100\n",
      "54/54 [==============================] - 24s 437ms/step - loss: 2.0289 - acc: 0.4444\n",
      "Epoch 4/100\n",
      "54/54 [==============================] - 24s 438ms/step - loss: 1.6184 - acc: 0.5185\n",
      "Epoch 5/100\n",
      "54/54 [==============================] - 24s 440ms/step - loss: 1.1430 - acc: 0.6667\n",
      "Epoch 6/100\n",
      "54/54 [==============================] - 24s 449ms/step - loss: 0.6523 - acc: 0.9630\n",
      "Epoch 7/100\n",
      "54/54 [==============================] - 23s 435ms/step - loss: 0.3871 - acc: 0.9444\n",
      "Epoch 8/100\n",
      "54/54 [==============================] - 24s 437ms/step - loss: 0.1772 - acc: 0.9815\n",
      "Epoch 9/100\n",
      "54/54 [==============================] - 24s 439ms/step - loss: 0.1220 - acc: 0.9815\n",
      "Epoch 10/100\n",
      "54/54 [==============================] - 24s 438ms/step - loss: 0.0809 - acc: 0.9815\n",
      "Epoch 11/100\n",
      "54/54 [==============================] - 24s 437ms/step - loss: 0.0683 - acc: 0.9815\n",
      "Epoch 12/100\n",
      "54/54 [==============================] - 23s 421ms/step - loss: 0.0430 - acc: 1.0000\n",
      "Epoch 13/100\n",
      " 3/54 [>.............................] - ETA: 21s - loss: 0.0307 - acc: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-2de1876ea52e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpadded_sequences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(padded_sequences, training_y, batch_size=1, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['hallo'], ['ist', 'jemand', 'da'], ['hi'], ['gut', 'tag'], ['hey'], ['moin'], ['servus'], ['wie', 'geht'], ['was', 'geht'], ['mir', 'geht', 'es', 'auch', 'gut'], ['mir', 'geht', 'es', 'sup'], ['es', 'geht', 'mir', 'gut'], ['gut'], ['sup'], ['ganz', 'ok'], ['nicht', 'so', 'gut'], ['es', 'geht', 'mir', 'schlecht'], ['schlecht'], ['wie', 'viel', 'sport'], ['wie', 'oft', 'sollt', 'ich', 'sport', 'mach'], ['wie', 'kann', 'ich', 'ein', 'sixpack', 'aufbau'], ['wie', 'bekommt', 'man', 'bauchmuskeln'], ['worauf', 'ist', 'zu', 'acht', 'wenn', 'man', 'mit', 'dem', 'krafttraining', 'anfangt'], ['ich', 'mocht', 'ins', 'fitnessstudio', 'worauf', 'muss', 'ich', 'acht'], ['was', 'ist', 'zu', 'beacht', 'wenn', 'man', 'mit', 'fitness', 'anfang', 'will'], ['was', 'ist', 'unt', 'gesund', 'ernahr', 'zu', 'versteh'], ['wie', 'muss', 'ich', 'mich', 'ernahr', 'um', 'gesund', 'zu', 'bleib'], ['gesund', 'nahrung'], ['wie', 'ernahr', 'ich', 'mich', 'gesund'], ['wie', 'lang', 'muss', 'ich', 'traini', 'um', 'erfolg', 'zu', 'seh'], ['wann', 'sieht', 'man', 'die', 'erst', 'ergebnis'], ['wie', 'kann', 'ich', 'mich', 'motivi', 'zum', 'training', 'zu', 'geh'], ['was', 'soll', 'ich', 'tun', 'wenn', 'ich', 'mal', 'kein', 'lust', 'hab', 'zum', 'training', 'zu', 'geh'], ['wie', 'kann', 'ich', 'mich', 'motivi', 'sport', 'zu', 'mach'], ['was', 'kann', 'ich', 'tun', 'um', 'abzunehm'], ['wie', 'kann', 'ich', 'gewicht', 'verli'], ['wie', 'werd', 'ich', 'schlank'], ['wie', 'nehm', 'ich', 'ab'], ['wie', 'bau', 'ich', 'muskeln', 'auf'], ['wie', 'nehm', 'ich', 'zu'], ['wie', 'kann', 'ich', 'gewicht', 'zunehm'], ['was', 'kann', 'ich', 'tun', 'um', 'zuzunehm'], ['dank'], ['viel', 'dank'], ['ich', 'bedank', 'mich'], ['tschuss'], ['tschuss'], ['bye'], ['caio'], ['auf', 'wiederseh'], ['bis', 'dann'], ['mach', 'gut'], ['bb'], ['bis', 'spat']]\n"
     ]
    }
   ],
   "source": [
    "print(training_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ich', 'mocht', 'gern', 'mit', 'fitness', 'anfang']]\n",
      "[2.0884080e-03 4.9062946e-04 7.2286103e-04 2.0856734e-03 2.0017901e-03\n",
      " 7.6918998e-03 8.8047105e-01 4.3495353e-02 1.3347347e-02 1.9683968e-04\n",
      " 3.3437867e-02 1.3619683e-02 2.2670202e-04 1.2379009e-04]\n",
      "anfangen\n",
      "6\n",
      "Wenn du mit dem Fitnesstraining beginnen möchtest, ist es sehr wichtig, dass du auf eine saubere Ausführung der Übungen achtest, um Verletzungen vorzubeugen. Einen für dich passenden Trainingsplan findest du unter ...de\n",
      "0.88047105\n"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "\n",
    "test = [\"ich möchte gerne mit fitness anfangen\"]\n",
    "preprocessed_test = []\n",
    "\n",
    "#Preprocessing\n",
    "for question in test:\n",
    "    #tokenize\n",
    "    tokens = nltk.word_tokenize(question, language='german')\n",
    "    #lowercase\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    #remove punctution\n",
    "    table = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    #remove non-alphabetic/non-numeric\n",
    "    real_tokens = [token for token in stripped if token.isalpha() or token.isnumeric()]\n",
    "    #stemming\n",
    "    sequence = [snowball.stem(token) for token in real_tokens]\n",
    "    \n",
    "    preprocessed_test.append(sequence)\n",
    "\n",
    "print(preprocessed_test)\n",
    "test_samples_tokens = tokenizer.texts_to_sequences(preprocessed_test)\n",
    "padded_samples = pad_sequences(test_samples_tokens, maxlen=300)\n",
    "\n",
    "results = model.predict(x=padded_samples)\n",
    "result_index = np.argmax(results)\n",
    "tag = labels[result_index]\n",
    "print(results[0])\n",
    "print(tag)\n",
    "print(result_index)\n",
    "\n",
    "for t in data[\"qa\"]:\n",
    "    if t[\"tag\"] == tag:\n",
    "        responses = t[\"answers\"]\n",
    "\n",
    "print(random.choice(responses))\n",
    "\n",
    "if (results[0][result_index] < 0.50):\n",
    "    print(\"ich weiß nicht\")\n",
    "else:\n",
    "    print(results[0][result_index])\n",
    "\n",
    "model.save('chatbot')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
